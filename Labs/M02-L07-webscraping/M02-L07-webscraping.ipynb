{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Web Scraping\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Web scraping can be defined as \"the construction of an agent to download, parse, and organize data from the web in an automated manner\". In this lab, you will practice a series of exercises to practice your web scraping skills.  \n",
    "\n",
    "Each exercise is independent from the previous one. If you get stuck in one exercise you can skip to the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "- Make sure you have all libraries installed before start the lab.  \n",
    "- In this lab you will use `requests`, `BeautifulSoup` and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping github trending developers\n",
    "- In this first exercise we will scraping the github trending developers. Use the url below.\n",
    "```python\n",
    "url = 'https://github.com/trending/developers'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start using `requests.get()` over the 'url', save your output in a new variable called `get_html`\n",
    "- The output should be `<Response [200]>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the request methods\n",
    "- Try get_html.status_code and get_html.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call the `get_html.content` method to return the page content.\n",
    "- Save in a variable called `html_content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the BeautifulSoup to parse your result. You can use the code below.\n",
    "```python\n",
    "soup = BeautifulSoup(html_content, \"lxml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "- Find out the html tag and class names used for the developer names.\n",
    "- Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "- Use string manipulation techniques to replace whitespaces and line breaks (i.e. \\n) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['KentC.Dodds',\n",
    " 'SethVargo',\n",
    " 'VadimDemedes',\n",
    " 'PaulBeusterien',\n",
    " 'DanImhoff',\n",
    " 'CalebPorzio',\n",
    " 'TannerLinsley',\n",
    " 'InesMontani',\n",
    " 'Mr.doob',\n",
    " 'JacobHoffman-Andrews',\n",
    " 'TianonGravi',\n",
    " 'TaylorOtwell',\n",
    " 'MatthewJohnson',\n",
    " 'MathiasBuus',\n",
    " 'TimHolman',\n",
    " 'AlonZakai',\n",
    " 'HadleyWickham',\n",
    " 'Bo-YiWu',\n",
    " 'TobiasKoppers',\n",
    " 'KentaroWada',\n",
    " 'TeppeiFukuda',\n",
    " 'MartinAtkins',\n",
    " 'RyanMcKinley',\n",
    " 'KlausPost',\n",
    " 'JamesAgnew']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function\n",
    "- Now you have learned how to use Requests and BeautifulSoup. \n",
    "- Create the function below to make your scraping easier.\n",
    "```python\n",
    "def url_bs4(url):\n",
    "    get_html = requests.get(url)\n",
    "    print(get_html.status_code)\n",
    "    print(get_html.encoding)\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Walt Disney wikipedia page\n",
    "- Use the url below to scraping the Walt Disney Wikipedia page.\n",
    "- Use the url_bs4 function and check the status.\n",
    "```python\n",
    "url_disney = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a list with  all the image links from Walt Disney Wikipedia page\n",
    "- Try the `.find_all` method to find the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping earthquakes\n",
    "- Use the url below to scraping the 50 latest earthquakes.\n",
    "```python\n",
    "url_eq='https://www.emsc-csem.org/Earthquake/'\n",
    "```\n",
    "- Instead  of use requests and BeautifulSoup,  try the function `pd.read_html(url_eq)`\n",
    "- You will notice that it returns a list of elements. One of the elements in this list is the earthquake table\n",
    "- You will need to clean the columns names, the Date & Time values,  and drop the last 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- Find the IMDB's Top 250 data.\n",
    "- You should have movie name, year release, director name and actors.\n",
    "- Create a dataframe with the data you collected.\n",
    "- Use the url below to this exercise.\n",
    "```python\n",
    "url_imdb = 'https://www.imdb.com/chart/top'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
